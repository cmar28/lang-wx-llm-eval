{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf569a7-9a1d-4489-934e-50e57760c907",
   "metadata": {},
   "source": [
    "# RAG - Search evaluation\n",
    "The quality of a RAG result heavily depends on the output of the \"retrieval\" step. This notebook shows how to use IBM watsonx and Langchain Evaluation framework to assess weather the result of a search is relevant to the search string.\n",
    "You find the \"RETRIEVAL_RELEVANCE\" criteria used to perform the evalution in [./utils/customer_criteria.py](./utils/custom_criteria.py). \n",
    "\n",
    "Please note that the criteria defined above is targeting the most common RAG use case, in which you are searching for an answer to a question. Your use case might be different. You should then customise the criteria to match your problem domain.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdacda01",
   "metadata": {},
   "source": [
    "## Create the evaluator factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29199ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from utils.wx_evaluator import wx_EvalFactory\n",
    "from utils.custom_criteria import CustomCriteria\n",
    "from langchain.evaluation import Criteria\n",
    "\n",
    "load_dotenv()\n",
    "url = os.environ.get(\"WATSONX_API_URL\")\n",
    "apikey = os.environ.get(\"WATSONX_API_KEY\")\n",
    "project_id = os.environ.get(\"WATSONX_PROJECT_ID\")\n",
    "\n",
    "credentials = {\n",
    "    \"url\": url,\n",
    "    \"apikey\": apikey\n",
    "}\n",
    "\n",
    "factory = wx_EvalFactory(credentials, project_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85031b66",
   "metadata": {},
   "source": [
    "## Configure logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f0326a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import langchain\n",
    "import sys\n",
    "\n",
    "log_levels = {\n",
    "    \"CRITICAL\": logging.CRITICAL,\n",
    "    \"ERROR\": logging.ERROR,\n",
    "    \"WARNING\": logging.WARNING,\n",
    "    \"INFO\": logging.INFO,\n",
    "    \"DEBUG\": logging.DEBUG,\n",
    "}\n",
    "\n",
    "LOG_LEVEL = log_levels[os.environ.get(\"LOG_LEVEL\", \"INFO\").upper()]\n",
    "\n",
    "if LOG_LEVEL == logging.DEBUG:\n",
    "    langchain.globals.set_debug(True)\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=LOG_LEVEL,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        #logging.FileHandler(\"app.log\"), \n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0419955",
   "metadata": {},
   "source": [
    "## Load search results\n",
    "This implementation assumes the search results to evaluate are stored in a .csv file in the ./data folder. The file has two columns: \"Input\" (the search query) and \"Output\" (the search result). Saving your searches in a file before evaluating them enables you to decouple the execution of the search from the evaluation. This is useful during the first evaluation cycles when you might want to test multiple evaluation approaches.\n",
    "\n",
    "When you are comfortable with the evaluation results you can extend the notebook to include the execution of the search too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd76c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file_path = './data/search_results.csv'\n",
    "strip_function = lambda x: x.strip() if isinstance(x, str) else x\n",
    "columns_to_strip = ['Input', 'Output']\n",
    "df = pd.read_csv(csv_file_path, converters={col: strip_function for col in columns_to_strip})\n",
    "df = df.dropna(axis=0, how='all')\n",
    "df = df.dropna(axis=1, how='all')\n",
    "records = df.to_dict('records')\n",
    "logging.debug(df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6918d85b",
   "metadata": {},
   "source": [
    "## Evaluate search results relevancy\n",
    "This section is running an evaluation chain for every example included in the dataset. The results are saved in a \"search_result_evaluation.csv\" file in the ./data folder.\n",
    "\n",
    "The chains are run in parallel to reduce the total execution time. You can control the level of parallelism through the \"n\" variable defined below (set to 10 by default).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "\n",
    "evaluator = factory.load_evaluator(CustomCriteria.RETRIEVAL_RELEVANCE)\n",
    "results = []\n",
    "\n",
    "# max number of parallel execution threads\n",
    "n = 10\n",
    "\n",
    "def evaluate_record(record):\n",
    "    logging.debug(record)\n",
    "    eval_result = evaluator.evaluate_strings(\n",
    "        prediction=record[\"Output\"],\n",
    "        input=record[\"Input\"],\n",
    "    )\n",
    "    logging.debug(eval_result)\n",
    "    return record, eval_result\n",
    "\n",
    "def evaluate_records_in_parallel(records, n):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=n) as executor:\n",
    "        # Submit all tasks to the executor\n",
    "        future_to_record = {executor.submit(evaluate_record, record): record for record in records}\n",
    "        try:\n",
    "            # As each task completes, process its result\n",
    "            for future in as_completed(future_to_record):\n",
    "                eval_record, result = future.result()  # This will raise any exceptions caught during the task execution\n",
    "                results.append({\n",
    "                    \"Input\": eval_record[\"Input\"],  \n",
    "                    \"value\": result[\"value\"],\n",
    "                    \"score\": result[\"score\"],\n",
    "                    \"reasoning\": result[\"reasoning\"]\n",
    "                })\n",
    "        except Exception as exc:\n",
    "            # If any exception occurs, log it and raise it to stop the processing\n",
    "            logging.error('A record generated an exception: %s' % exc)\n",
    "            executor.shutdown(wait=False)  # Immediately stop all executing tasks\n",
    "            raise  # Reraise the exception to indicate failure\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = evaluate_records_in_parallel(records, n)\n",
    "results_dict = {result[\"Input\"]: result for result in results}\n",
    "    \n",
    "# Initialize new columns with default values or NaNs\n",
    "df['Eval value'] = pd.NA\n",
    "df['Eval score'] = pd.NA\n",
    "df['Eval reasoning'] = pd.NA\n",
    "    \n",
    "# Iterate over DataFrame to set values based on \"Input\"\n",
    "for idx, row in df.iterrows():\n",
    "    input_val = row[\"Input\"]\n",
    "    if input_val in results_dict:\n",
    "        df.at[idx, 'Eval value'] = results_dict[input_val][\"value\"]\n",
    "        df.at[idx, 'Eval score'] = results_dict[input_val][\"score\"]\n",
    "        df.at[idx, 'Eval reasoning'] = results_dict[input_val][\"reasoning\"]\n",
    "  \n",
    "# Getting the current date and time\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Constructing the new file name with the current date and time and the original csv_file_path\n",
    "eval_file_path = csv_file_path.replace('.csv', f\"_evaluation_{current_datetime}.csv\")\n",
    "df.to_csv(eval_file_path, index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
