{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf569a7-9a1d-4489-934e-50e57760c907",
   "metadata": {},
   "source": [
    "# Criteria Evaluation\n",
    "Langchain evaluation module supports the scenarios in which you want to assess generative ai outputs using a specific rubric or criteria set.\n",
    "The assessment is performed by an LLM playing the role of \"Evaluator\". To understand the langchain functionality and configurability in depth, refer to the reference documentation of the [CriteriaEvalChain](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.criteria.eval_chain.CriteriaEvalChain.html#langchain.evaluation.criteria.eval_chain.CriteriaEvalChain) class.\n",
    "\n",
    "IBM watsonx foundation models are among the list of LLM models [supported by Langchain](https://python.langchain.com/docs/integrations/llms/watsonxllm). \n",
    "\n",
    "This notebook shows how to use fondation models running on IBM watsonx as criteria evaluators to assess generative AI outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdacda01",
   "metadata": {},
   "source": [
    "## Setup a watsonx model as the Evaluator\n",
    "If you haven't used watsonx.ai before, please refer to the documentation at this [link](https://python.langchain.com/docs/integrations/llms/ibm_watsonx), and the included references on how to create projects and keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29199ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "url = os.environ.get(\"WATSONX_API_URL\")\n",
    "apikey = os.environ.get(\"WATSONX_API_KEY\")\n",
    "project_id = os.environ.get(\"WATSONX_PROJECT_ID\")\n",
    "\n",
    "credentials = {\n",
    "    \"url\": url,\n",
    "    \"apikey\": apikey\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a57fed8",
   "metadata": {},
   "source": [
    "The setup below configures \"Lllama2 70B\" chat as the evaluator. You can select other models by changing the model_id value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f83a1df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "generate_params = {\n",
    "    #customise model parameters here\n",
    "    GenParams.MAX_NEW_TOKENS: 4000\n",
    "}\n",
    "\n",
    "\n",
    "model_id = \"ibm-mistralai/mixtral-8x7b-instruct-v01-q\"\n",
    "\n",
    "model = Model (\n",
    "    model_id=model_id,\n",
    "    credentials=credentials,\n",
    "    params=generate_params,\n",
    "    project_id=project_id,\n",
    ")\n",
    "\n",
    "\n",
    "wx_llm = WatsonxLLM(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bb79b1",
   "metadata": {},
   "source": [
    "When you call the langchain \"load_evaluator\" method, always specify the parameter llm=wx_llm to use the model configured above as the evaluator "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85031b66",
   "metadata": {},
   "source": [
    "The code snippet below configures your logging. Set LOG_LEVEL=DEBUG in your .env file to gain better insights the interaction between Langchain components and the LLM performing the evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f0326a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import langchain\n",
    "import sys\n",
    "\n",
    "log_levels = {\n",
    "    \"CRITICAL\": logging.CRITICAL,\n",
    "    \"ERROR\": logging.ERROR,\n",
    "    \"WARNING\": logging.WARNING,\n",
    "    \"INFO\": logging.INFO,\n",
    "    \"DEBUG\": logging.DEBUG,\n",
    "}\n",
    "\n",
    "LOG_LEVEL = log_levels[os.environ.get(\"LOG_LEVEL\", \"INFO\").upper()]\n",
    "\n",
    "if LOG_LEVEL == logging.DEBUG:\n",
    "    langchain.globals.set_debug(True)\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=LOG_LEVEL,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        #logging.FileHandler(\"app.log\"), \n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1752a",
   "metadata": {},
   "source": [
    "## Review the evaluator prompt template\n",
    "The prompt template used by the LLM evaluator can have a significant impact on the result of the evaluation.\n",
    "Langchain provides default evaluation prompt templates based on OpenAI gpt-4.\n",
    "However different LLM evaluators might interpret the template differently. You should make a conscious decision weather to use the default or customise the template.\n",
    "\n",
    "The evaluator template comes in two flavours: one for criteria that do not require reference data (for example \"conciseness\") and another for criteria that do require reference data (for example \"correctness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ad7ad",
   "metadata": {},
   "source": [
    "You find the default templates below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c43ca259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\n",
    "[BEGIN DATA]\n",
    "***\n",
    "[Input]: {input}\n",
    "***\n",
    "[Submission]: {output}\n",
    "***\n",
    "[Criteria]: {criteria}\n",
    "***\n",
    "[END DATA]\n",
    "Does the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\"\"\"\n",
    "\n",
    "DEF_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\", \"criteria\"], template=template\n",
    ")\n",
    "\n",
    "template = \"\"\"You are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\n",
    "[BEGIN DATA]\n",
    "***\n",
    "[Input]: {input}\n",
    "***\n",
    "[Submission]: {output}\n",
    "***\n",
    "[Criteria]: {criteria}\n",
    "***\n",
    "[Reference]: {reference}\n",
    "***\n",
    "[END DATA]\n",
    "Does the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\"\"\"\n",
    "\n",
    "DEF_PROMPT_WITH_REFERENCES = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\", \"criteria\", \"reference\"], template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d68b479",
   "metadata": {},
   "source": [
    "...and here is a version customised to use Llama2-70B-chat or mixtral-8x7b-instuctruct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b7f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"<s>[INST]<<SYS>> You are a criteria evaluator assessing whether a submission, based on an input, is compliant with a set of criteria. \n",
    "Your output includes a step by step reasoning and an overall answer, in the following format:\n",
    "\n",
    "Step by step reasoning: <Does the submission meet the Criteria? Write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct>\n",
    "\n",
    "Answer:  <a single character \"Y\" or \"N\" (without quotes or punctuation). \"Y\" if the submission meets all the criteria, \"N\" if the submission doesn't meet one or more of the criteria.>\n",
    "<</SYS>>\n",
    "\n",
    "[Input]: {input}\n",
    "\n",
    "[Submission]: {output}\n",
    "\n",
    "[Criteria]: {criteria}\n",
    "\n",
    "Respond only with \"Step by step reasoning\" and \"Answer\"\n",
    "[/INST] \"\"\"\n",
    "\n",
    "OS_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\", \"criteria\"], template=template\n",
    ")\n",
    "\n",
    "template = \"\"\"<s>[INST]<<SYS>> You are a criteria evaluator assessing whether a submission, based on an input, is compliant with a set of criteria. Base your answer only on the reference data provided.\n",
    "Your output includes a step by step reasoning and an overall answer, in the following format:\n",
    "\n",
    "Step by step reasoning: <Does the submission meet the Criteria, on the basis of the reference? Write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct>\n",
    "\n",
    "Answer: <a single character \"Y\" or \"N\" (without quotes or punctuation), based on the step by step reasoning>\n",
    "<</SYS>>\n",
    "\n",
    "[Input]: {input}\n",
    "\n",
    "[Submission]: {output}\n",
    "\n",
    "[Criteria]: {criteria}\n",
    "\n",
    "[Reference]: {reference}\n",
    "\n",
    "Respond only with \"Step by step reasoning\" and \"Answer\"\n",
    "[/INST] \"\"\"\n",
    "\n",
    "OS_PROMPT_WITH_REFERENCES = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\", \"criteria\", \"reference\"], template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c696e",
   "metadata": {},
   "source": [
    "When calling langchain load_evaluator method, use the \"prompt\" variable to specify custom evaluation templates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa04ea2",
   "metadata": {},
   "source": [
    "## Criteria evaluation without references\n",
    "\n",
    " All string evaluators expose an [evaluate_strings](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.criteria.eval_chain.CriteriaEvalChain.html?highlight=evaluate_strings#langchain.evaluation.criteria.eval_chain.CriteriaEvalChain.evaluate_strings) (or async [aevaluate_strings](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.criteria.eval_chain.CriteriaEvalChain.html?highlight=evaluate_strings#langchain.evaluation.criteria.eval_chain.CriteriaEvalChain.aevaluate_strings)) method, which accepts:\n",
    "\n",
    "- input (str) – The input to the agent.\n",
    "- prediction (str) – The predicted response.\n",
    "\n",
    "The criteria evaluators return a dictionary with the following values:\n",
    "- score: Binary integer 0 to 1, where 1 would mean that the output is compliant with the criteria, and 0 otherwise\n",
    "- value: A \"Y\" or \"N\" corresponding to the score\n",
    "- reasoning: String \"chain of thought reasoning\" from the LLM generated prior to creating the score\n",
    "\n",
    "In this example, you will use the `CriteriaEvalChain` to check whether an output is concise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6005ebe8-551e-47a5-b4df-80575a068552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.evaluation import Criteria\n",
    "\n",
    "template = OS_PROMPT\n",
    "\n",
    "\n",
    "evaluator = load_evaluator(EvaluatorType.CRITERIA, llm=wx_llm,prompt=template, criteria=Criteria.CONCISENESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22f83fb8-82f4-4310-a877-68aaa0789199",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:CriteriaEvalChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What's your name?\",\n",
      "  \"output\": \"That's a difficult question. People call me Hamlet, but I am not sure I like it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:CriteriaEvalChain > 2:llm:WatsonxLLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"<s>[INST]<<SYS>> You are a criteria evaluator assessing whether a submission, based on an input, is compliant with a set of criteria. \\nYour output includes a step by step reasoning and an overall answer, in the following format:\\n\\nStep by step reasoning: <Does the submission meet the Criteria? Write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct>\\n\\nAnswer:  <a single character \\\"Y\\\" or \\\"N\\\" (without quotes or punctuation). \\\"Y\\\" if the submission meets all the criteria, \\\"N\\\" if the submission doesn't meet one or more of the criteria.>\\n<</SYS>>\\n\\n[Input]: What's your name?\\n\\n[Submission]: That's a difficult question. People call me Hamlet, but I am not sure I like it\\n\\n[Criteria]: conciseness: Is the submission concise and to the point?\\n\\nRespond only with \\\"Step by step reasoning\\\" and \\\"Answer\\\"\\n[/INST]\"\n",
      "  ]\n",
      "}\n",
      "2024-02-19 15:44:55,351 [INFO] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text?version=2024-01-25'\n",
      "2024-02-19 15:44:55,352 [DEBUG] Response(POST https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text?version=2024-01-25): {\"model_id\":\"ibm-mistralai/mixtral-8x7b-instruct-v01-q\",\"created_at\":\"2024-02-19T15:44:55.293Z\",\"results\":[{\"generated_text\":\"\\nStep by step reasoning: The submission does answer the question of what the submitter's name is, however it does so in a roundabout way. The question asked was for a name, not a statement about the name or a preference for it. Therefore, the submission is not concise and to the point.\\n\\nAnswer: N\",\"generated_token_count\":72,\"input_token_count\":231,\"stop_reason\":\"eos_token\"}],\"system\":{\"warnings\":[{\"message\":\"This model is a Non-IBM Product governed by a third-party license that may impose use restrictions and other obligations. By using this model you agree to its terms as identified in the following URL.\",\"id\":\"disclaimer_warning\",\"more_info\":\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx\"}]}}\n",
      "2024-02-19 15:44:55,354 [INFO] Output of watsonx.ai call: \n",
      "Step by step reasoning: The submission does answer the question of what the submitter's name is, however it does so in a roundabout way. The question asked was for a name, not a statement about the name or a preference for it. Therefore, the submission is not concise and to the point.\n",
      "\n",
      "Answer: N\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:CriteriaEvalChain > 2:llm:WatsonxLLM] [4.71s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\nStep by step reasoning: The submission does answer the question of what the submitter's name is, however it does so in a roundabout way. The question asked was for a name, not a statement about the name or a preference for it. Therefore, the submission is not concise and to the point.\\n\\nAnswer: N\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:CriteriaEvalChain] [4.71s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"results\": {\n",
      "    \"reasoning\": \"Step by step reasoning: The submission does answer the question of what the submitter's name is, however it does so in a roundabout way. The question asked was for a name, not a statement about the name or a preference for it. Therefore, the submission is not concise and to the point.\\n\\nAnswer:\",\n",
      "    \"value\": \"N\",\n",
      "    \"score\": 0\n",
      "  }\n",
      "}\n",
      "2024-02-19 15:44:55,354 [INFO] {'reasoning': \"Step by step reasoning: The submission does answer the question of what the submitter's name is, however it does so in a roundabout way. The question asked was for a name, not a statement about the name or a preference for it. Therefore, the submission is not concise and to the point.\\n\\nAnswer:\", 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"That's a difficult question. People call me Hamlet, but I am not sure I like it\",\n",
    "    input=\"What's your name?\",\n",
    ")\n",
    "logging.info(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40b1ac7-8f95-48ed-89a2-623bcc746461",
   "metadata": {},
   "source": [
    "## Criteria evaluation with reference labels\n",
    "\n",
    "Some criteria (such as correctness) require reference labels to work correctly. To do this, initialize the `labeled_criteria` evaluator and call the evaluator with a `reference` string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d8a86b-beba-42ce-b82c-d9e5ebc13686",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LabeledCriteriaEvalChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the capital of the US?\",\n",
      "  \"output\": \"Topeka, KS\",\n",
      "  \"reference\": \"The capital of the US is Topeka, KS, where it permanently moved from Washington D.C. on May 16, 2023\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LabeledCriteriaEvalChain > 2:llm:WatsonxLLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"<s>[INST]<<SYS>> You are a criteria evaluator assessing whether a submission, based on an input, is compliant with a set of criteria. Base your answer only on the reference data provided.\\nYour output includes a step by step reasoning and an overall answer, in the following format:\\n\\nStep by step reasoning: <Does the submission meet the Criteria, on the basis of the reference? Write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct>\\n\\nAnswer: <a single character \\\"Y\\\" or \\\"N\\\" (without quotes or punctuation), based on the step by step reasoning>\\n<</SYS>>\\n\\n[Input]: What is the capital of the US?\\n\\n[Submission]: Topeka, KS\\n\\n[Criteria]: correctness: Is the submission correct, accurate, and factual?\\n\\n[Reference]: The capital of the US is Topeka, KS, where it permanently moved from Washington D.C. on May 16, 2023\\n\\nRespond only with \\\"Step by step reasoning\\\" and \\\"Answer\\\"\\n[/INST]\"\n",
      "  ]\n",
      "}\n",
      "2024-02-19 15:45:21,681 [INFO] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text?version=2024-01-25'\n",
      "2024-02-19 15:45:21,682 [DEBUG] Response(POST https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text?version=2024-01-25): {\"model_id\":\"ibm-mistralai/mixtral-8x7b-instruct-v01-q\",\"created_at\":\"2024-02-19T15:45:21.618Z\",\"results\":[{\"generated_text\":\"1. The criterion to be evaluated is correctness, which requires the submission to be accurate and factual.\\n2. The reference states that the capital of the US is Topeka, KS, where it permanently moved from Washington D.C. on May 16, 2023.\\n3. The submission is \\\"Topeka, KS\\\".\\n4. Comparing the submission to the reference, the submission is in agreement with the reference.\\n\\nAnswer: Y\",\"generated_token_count\":107,\"input_token_count\":251,\"stop_reason\":\"eos_token\"}],\"system\":{\"warnings\":[{\"message\":\"This model is a Non-IBM Product governed by a third-party license that may impose use restrictions and other obligations. By using this model you agree to its terms as identified in the following URL.\",\"id\":\"disclaimer_warning\",\"more_info\":\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx\"}]}}\n",
      "2024-02-19 15:45:21,685 [INFO] Output of watsonx.ai call: 1. The criterion to be evaluated is correctness, which requires the submission to be accurate and factual.\n",
      "2. The reference states that the capital of the US is Topeka, KS, where it permanently moved from Washington D.C. on May 16, 2023.\n",
      "3. The submission is \"Topeka, KS\".\n",
      "4. Comparing the submission to the reference, the submission is in agreement with the reference.\n",
      "\n",
      "Answer: Y\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LabeledCriteriaEvalChain > 2:llm:WatsonxLLM] [6.54s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"1. The criterion to be evaluated is correctness, which requires the submission to be accurate and factual.\\n2. The reference states that the capital of the US is Topeka, KS, where it permanently moved from Washington D.C. on May 16, 2023.\\n3. The submission is \\\"Topeka, KS\\\".\\n4. Comparing the submission to the reference, the submission is in agreement with the reference.\\n\\nAnswer: Y\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LabeledCriteriaEvalChain] [6.54s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"results\": {\n",
      "    \"reasoning\": \"1. The criterion to be evaluated is correctness, which requires the submission to be accurate and factual.\\n2. The reference states that the capital of the US is Topeka, KS, where it permanently moved from Washington D.C. on May 16, 2023.\\n3. The submission is \\\"Topeka, KS\\\".\\n4. Comparing the submission to the reference, the submission is in agreement with the reference.\\n\\nAnswer:\",\n",
      "    \"value\": \"Y\",\n",
      "    \"score\": 1\n",
      "  }\n",
      "}\n",
      "2024-02-19 15:45:21,687 [INFO] {'reasoning': '1. The criterion to be evaluated is correctness, which requires the submission to be accurate and factual.\\n2. The reference states that the capital of the US is Topeka, KS, where it permanently moved from Washington D.C. on May 16, 2023.\\n3. The submission is \"Topeka, KS\".\\n4. Comparing the submission to the reference, the submission is in agreement with the reference.\\n\\nAnswer:', 'value': 'Y', 'score': 1}\n"
     ]
    }
   ],
   "source": [
    "template = OS_PROMPT_WITH_REFERENCES\n",
    "\n",
    "evaluator = load_evaluator(EvaluatorType.LABELED_CRITERIA, llm=wx_llm, prompt=template, criteria=Criteria.CORRECTNESS)\n",
    "\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    input=\"What is the capital of the US?\",\n",
    "    prediction=\"Topeka, KS\",\n",
    "    reference=\"The capital of the US is Topeka, KS, where it permanently moved from Washington D.C. on May 16, 2023\",\n",
    ")\n",
    "\n",
    "logging.info(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b5748-d373-4ff8-85d9-21da4641e84c",
   "metadata": {},
   "source": [
    "**Default Criteria**\n",
    "\n",
    "You find below a list of pre-implemented criteria. Note that in the absence of labels, the LLM merely predicts what it thinks the best answer is and is not grounded on a reliable context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47de7359-db3e-4cad-bcfa-4fe834dea893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Criteria.CONCISENESS: 'conciseness'>,\n",
       " <Criteria.RELEVANCE: 'relevance'>,\n",
       " <Criteria.CORRECTNESS: 'correctness'>,\n",
       " <Criteria.COHERENCE: 'coherence'>,\n",
       " <Criteria.HARMFULNESS: 'harmfulness'>,\n",
       " <Criteria.MALICIOUSNESS: 'maliciousness'>,\n",
       " <Criteria.HELPFULNESS: 'helpfulness'>,\n",
       " <Criteria.CONTROVERSIALITY: 'controversiality'>,\n",
       " <Criteria.MISOGYNY: 'misogyny'>,\n",
       " <Criteria.CRIMINALITY: 'criminality'>,\n",
       " <Criteria.INSENSITIVITY: 'insensitivity'>,\n",
       " <Criteria.DEPTH: 'depth'>,\n",
       " <Criteria.CREATIVITY: 'creativity'>,\n",
       " <Criteria.DETAIL: 'detail'>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import Criteria\n",
    "\n",
    "# For a list of other default supported criteria, try calling `supported_default_criteria`\n",
    "list(Criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c4715-e857-44a3-9f87-346642586a8d",
   "metadata": {},
   "source": [
    "## Custom Criteria\n",
    "\n",
    "To evaluate outputs against your own custom criteria, or to be more explicit the definition of any of the default criteria, pass in a dictionary of `\"criterion_name\": \"criterion_description\"`\n",
    "\n",
    "Note: it's recommended that you create a single evaluator per criterion. This way, separate feedback can be provided for each aspect. Additionally, if you provide antagonistic criteria, the evaluator won't be very useful, as it will be configured to predict compliance for ALL of the criteria provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bafa0a11-2617-4663-84bf-24df7d0736be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:CriteriaEvalChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Tell me a joke\",\n",
      "  \"output\": \"I ate some square pie but I don't know the square of pi.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:CriteriaEvalChain > 2:llm:WatsonxLLM] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"<s>[INST]<<SYS>> You are a criteria evaluator assessing whether a submission, based on an input, is compliant with a set of criteria. \\nYour output includes a step by step reasoning and an overall answer, in the following format:\\n\\nStep by step reasoning: <Does the submission meet the Criteria? Write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct>\\n\\nAnswer:  <a single character \\\"Y\\\" or \\\"N\\\" (without quotes or punctuation). \\\"Y\\\" if the submission meets all the criteria, \\\"N\\\" if the submission doesn't meet one or more of the criteria.>\\n<</SYS>>\\n\\n[Input]: Tell me a joke\\n\\n[Submission]: I ate some square pie but I don't know the square of pi.\\n\\n[Criteria]: numeric: Does the output contain numeric or mathematical information?\\n\\nRespond only with \\\"Step by step reasoning\\\" and \\\"Answer\\\"\\n[/INST]\"\n",
      "  ]\n",
      "}\n",
      "2024-02-19 15:45:51,235 [INFO] Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text?version=2024-01-25'\n",
      "2024-02-19 15:45:51,237 [DEBUG] Response(POST https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text?version=2024-01-25): {\"model_id\":\"ibm-mistralai/mixtral-8x7b-instruct-v01-q\",\"created_at\":\"2024-02-19T15:45:51.172Z\",\"results\":[{\"generated_text\":\"\\nStep by step reasoning: \\nThe submission is \\\"I ate some square pie but I don't know the square of pi.\\\" The phrase \\\"square pie\\\" is not a numerical or mathematical term, but rather a play on words with the colloquial term \\\"square meal.\\\" The submission does not contain any numerical or mathematical information, so it does not meet the criterion of containing numeric or mathematical information.\\n\\nAnswer: N\",\"generated_token_count\":94,\"input_token_count\":223,\"stop_reason\":\"eos_token\"}],\"system\":{\"warnings\":[{\"message\":\"This model is a Non-IBM Product governed by a third-party license that may impose use restrictions and other obligations. By using this model you agree to its terms as identified in the following URL.\",\"id\":\"disclaimer_warning\",\"more_info\":\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx\"}]}}\n",
      "2024-02-19 15:45:51,240 [INFO] Output of watsonx.ai call: \n",
      "Step by step reasoning: \n",
      "The submission is \"I ate some square pie but I don't know the square of pi.\" The phrase \"square pie\" is not a numerical or mathematical term, but rather a play on words with the colloquial term \"square meal.\" The submission does not contain any numerical or mathematical information, so it does not meet the criterion of containing numeric or mathematical information.\n",
      "\n",
      "Answer: N\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:CriteriaEvalChain > 2:llm:WatsonxLLM] [5.61s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\nStep by step reasoning: \\nThe submission is \\\"I ate some square pie but I don't know the square of pi.\\\" The phrase \\\"square pie\\\" is not a numerical or mathematical term, but rather a play on words with the colloquial term \\\"square meal.\\\" The submission does not contain any numerical or mathematical information, so it does not meet the criterion of containing numeric or mathematical information.\\n\\nAnswer: N\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:CriteriaEvalChain] [5.61s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"results\": {\n",
      "    \"reasoning\": \"Step by step reasoning: \\nThe submission is \\\"I ate some square pie but I don't know the square of pi.\\\" The phrase \\\"square pie\\\" is not a numerical or mathematical term, but rather a play on words with the colloquial term \\\"square meal.\\\" The submission does not contain any numerical or mathematical information, so it does not meet the criterion of containing numeric or mathematical information.\\n\\nAnswer:\",\n",
      "    \"value\": \"N\",\n",
      "    \"score\": 0\n",
      "  }\n",
      "}\n",
      "2024-02-19 15:45:51,242 [INFO] {'reasoning': 'Step by step reasoning: \\nThe submission is \"I ate some square pie but I don\\'t know the square of pi.\" The phrase \"square pie\" is not a numerical or mathematical term, but rather a play on words with the colloquial term \"square meal.\" The submission does not contain any numerical or mathematical information, so it does not meet the criterion of containing numeric or mathematical information.\\n\\nAnswer:', 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "template = OS_PROMPT\n",
    "\n",
    "custom_criterion = {\n",
    "    \"numeric\": \"Does the output contain numeric or mathematical information?\"\n",
    "}\n",
    "\n",
    "eval_chain = load_evaluator(\n",
    "    EvaluatorType.CRITERIA,\n",
    "    llm=wx_llm,\n",
    "    prompt=template,\n",
    "    criteria=custom_criterion,\n",
    ")\n",
    "\n",
    "query = \"Tell me a joke\"\n",
    "prediction = \"I ate some square pie but I don't know the square of pi.\"\n",
    "eval_result = eval_chain.evaluate_strings(prediction=prediction, input=query)\n",
    "logging.info(eval_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a684e2f1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
